{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4dcc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba494fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d98a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbad5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751e69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    " model = Sequential([\n",
    " Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    " MaxPooling2D(pool_size=(2, 2)),\n",
    " Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    " MaxPooling2D(pool_size=(2, 2)),\n",
    " Flatten(),\n",
    " Dense(512, activation='relu'),\n",
    " Dropout(0.5),\n",
    " Dense(10, activation='softmax')\n",
    " ])\n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a7773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, num_epochs):\n",
    " model = create_cnn_model()\n",
    " optimizer = Adam(learning_rate=learning_rate)\n",
    " model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " history = model.fit(X_train, y_train, batch_size=128, epochs=num_epochs, validation_split=0.1)\n",
    " return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea67f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "352/352 [==============================] - 18s 49ms/step - loss: 1.5811 - accuracy: 0.4267 - val_loss: 1.2514 - val_accuracy: 0.5568\n",
      "Epoch 2/10\n",
      "352/352 [==============================] - 18s 50ms/step - loss: 1.2258 - accuracy: 0.5637 - val_loss: 1.0817 - val_accuracy: 0.6228\n",
      "Epoch 3/10\n",
      "352/352 [==============================] - 18s 50ms/step - loss: 1.0725 - accuracy: 0.6233 - val_loss: 0.9607 - val_accuracy: 0.6694\n",
      "Epoch 4/10\n",
      "352/352 [==============================] - 17s 50ms/step - loss: 0.9648 - accuracy: 0.6621 - val_loss: 0.9086 - val_accuracy: 0.6820\n",
      "Epoch 5/10\n",
      "352/352 [==============================] - 17s 49ms/step - loss: 0.8922 - accuracy: 0.6865 - val_loss: 0.8510 - val_accuracy: 0.7080\n",
      "Epoch 6/10\n",
      "352/352 [==============================] - 17s 49ms/step - loss: 0.8186 - accuracy: 0.7144 - val_loss: 0.8297 - val_accuracy: 0.7140\n",
      "Epoch 7/10\n",
      "352/352 [==============================] - 17s 49ms/step - loss: 0.7639 - accuracy: 0.7338 - val_loss: 0.8438 - val_accuracy: 0.7114\n",
      "Epoch 8/10\n",
      "352/352 [==============================] - 17s 49ms/step - loss: 0.7057 - accuracy: 0.7533 - val_loss: 0.7809 - val_accuracy: 0.7298\n",
      "Epoch 9/10\n",
      "352/352 [==============================] - 18s 50ms/step - loss: 0.6511 - accuracy: 0.7730 - val_loss: 0.7599 - val_accuracy: 0.7366\n",
      "Epoch 10/10\n",
      "352/352 [==============================] - 17s 49ms/step - loss: 0.6037 - accuracy: 0.7882 - val_loss: 0.7457 - val_accuracy: 0.7510\n",
      "Test Loss: 0.7725\n",
      "Test Accuracy: 0.7396\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model):\n",
    " loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    " print(f'Test Loss: {loss:.4f}')\n",
    " print(f'Test Accuracy: {accuracy:.4f}')\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "history, model = train_model(learning_rate, num_epochs)\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c42cd482",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],   label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],   label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'],   label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'],   label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f7314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Certainly! Let's break down the provided code step by step:\n",
    "##\n",
    "##1. **Loading the CIFAR-10 Dataset**:\n",
    "##    - `from tensorflow.keras.datasets import cifar10`: Imports the CIFAR-10 dataset, which contains 60,000 32x32 color images in 10 different classes (e.g., airplanes, automobiles, birds, cats, etc.).\n",
    "##    - `(X_train, y_train), (X_test, y_test) = cifar10.load_data()`: Loads the CIFAR-10 dataset, splitting it into training and testing sets. `X_train` and `X_test` contain the image data, while `y_train` and `y_test` contain the corresponding class labels.\n",
    "##\n",
    "##2. **Data Pre-processing**:\n",
    "##    - `X_train = X_train.astype('float32') / 255`: Normalizes the pixel values of the training images to the range [0, 1].\n",
    "##    - `X_test = X_test.astype('float32') / 255`: Similar normalization for the test images.\n",
    "##    - `y_train = to_categorical(y_train, 10)`: Converts the training labels to one-hot encoded format (since there are 10 classes).\n",
    "##    - `y_test = to_categorical(y_test, 10)`: Converts the test labels to one-hot encoded format.\n",
    "##\n",
    "##3. **Defining the CNN Model**:\n",
    "##    - `def create_cnn_model()`: Defines a function that creates a CNN model.\n",
    "##    - Inside the model:\n",
    "##        - `Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3))`: Adds a 2D convolutional layer with 32 filters, ReLU activation, and input shape (32x32x3).\n",
    "##        - `MaxPooling2D(pool_size=(2, 2))`: Adds a max-pooling layer with a 2x2 pool size.\n",
    "##        - `Conv2D(64, kernel_size=(3, 3), activation='relu')`: Adds another convolutional layer with 64 filters and ReLU activation.\n",
    "##        - `MaxPooling2D(pool_size=(2, 2))`: Adds another max-pooling layer.\n",
    "##        - `Flatten()`: Flattens the output from the convolutional layers.\n",
    "##        - `Dense(512, activation='relu')`: Adds a fully connected layer with 512 neurons and ReLU activation.\n",
    "##        - `Dropout(0.5)`: Adds dropout regularization to prevent overfitting.\n",
    "##        - `Dense(10, activation='softmax')`: Adds the output layer with 10 neurons (for 10 classes) and softmax activation.\n",
    "##\n",
    "##4. **Compiling and Training the Model**:\n",
    "##    - `def train_model(learning_rate, num_epochs)`: Defines a function that trains the model.\n",
    "##    - Inside the function:\n",
    "##        - `model = create_cnn_model()`: Creates the CNN model.\n",
    "##        - `optimizer = Adam(learning_rate=learning_rate)`: Initializes the Adam optimizer with the specified learning rate.\n",
    "##        - `model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])`: Compiles the model, specifying categorical cross-entropy loss and accuracy as the evaluation metric.\n",
    "##        - `history = model.fit(X_train, y_train, batch_size=128, epochs=num_epochs, validation_split=0.1)`: Trains the model using the training data. It runs for `num_epochs` epochs with a batch size of 128 and validates on a 10% validation split.\n",
    "##\n",
    "##5. **Evaluating Model Performance**:\n",
    "##    - `def evaluate_model(model)`: Defines a function that evaluates the model.\n",
    "##    - Inside the function:\n",
    "##        - `loss, accuracy = model.evaluate(X_test, y_test, verbose=0)`: Computes the test loss and accuracy.\n",
    "##        - Prints the test loss and accuracy.\n",
    "##\n",
    "##This code demonstrates building and training a CNN model for image classification using the CIFAR-10 dataset. The model architecture includes convolutional layers, max-pooling, dropout, and fully connected layers. #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc746b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8f151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
