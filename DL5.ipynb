{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cf9e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, Reshape, Conv2DTranspose, BatchNormalization, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97912a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "(train_x, _), (_, _) = fashion_mnist.load_data()\n",
    "train_x = (train_x / 255.) * 2 - 1\n",
    "train_x = train_x.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86f8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "generator = Sequential([\n",
    " Dense(512, input_shape=[100]),\n",
    " LeakyReLU(alpha=0.2),\n",
    " BatchNormalization(momentum=0.8),\n",
    " Dense(256),\n",
    " LeakyReLU(alpha=0.2),\n",
    " BatchNormalization(momentum=0.8),\n",
    " Dense(128),\n",
    " LeakyReLU(alpha=0.2),\n",
    " BatchNormalization(momentum=0.8),\n",
    " Dense(784),\n",
    " Reshape([28, 28, 1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae97e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create discriminator\n",
    "discriminator = Sequential([\n",
    " Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1)),\n",
    " LeakyReLU(alpha=0.2),\n",
    " Dropout(0.3),\n",
    " Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    " LeakyReLU(alpha=0.2),\n",
    " Dropout(0.3),\n",
    " Flatten(),\n",
    " Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GAN\n",
    "GAN = Sequential([generator, discriminator])\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "discriminator.trainable = False\n",
    "GAN.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f42086cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 5\n",
    "batch_size = 100\n",
    "noise_shape = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37e04f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently on Epoch 1\n",
      "Currently on Epoch 2\n",
      "Currently on Epoch 3\n",
      "Currently on Epoch 4\n",
      "Currently on Epoch 5\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    " print(f\"Currently on Epoch {epoch + 1}\")\n",
    " for i in range(train_x.shape[0] // batch_size):\n",
    "   noise = np.random.normal(size=[batch_size, noise_shape])\n",
    " gen_image = generator.predict_on_batch(noise)\n",
    " train_dataset = train_x[i * batch_size:(i + 1) * batch_size]\n",
    " train_label = np.ones(shape=(batch_size, 1))\n",
    " discriminator.trainable = True\n",
    " d_loss_real = discriminator.train_on_batch(train_dataset, train_label)\n",
    " train_label = np.zeros(shape=(batch_size, 1))\n",
    " d_loss_fake = discriminator.train_on_batch(gen_image, train_label)\n",
    " noise = np.random.normal(size=[batch_size, noise_shape])\n",
    " train_label = np.ones(shape=(batch_size, 1))\n",
    " discriminator.trainable = False\n",
    " d_g_loss_batch = GAN.train_on_batch(noise, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "937f8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting generated images at the start and then after every 10 epoch\n",
    "if epoch % 10 == 0:\n",
    "    samples = 10\n",
    "    x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, 100)))\n",
    "    for k in range(samples):\n",
    "        plt.subplot(2, 5, k + 1)\n",
    "        plt.imshow(x_fake[k].reshape(28, 28), cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print('Training is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Sure, let's break down the provided code line by line:\n",
    "#\n",
    "#```python\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from keras.datasets import fashion_mnist\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Conv2D, Dropout, Flatten, Reshape, Conv2DTranspose, BatchNormalization, LeakyReLU\n",
    "#```\n",
    "#\n",
    "#- The code begins with importing necessary libraries such as NumPy for numerical computations, Matplotlib for visualization, and Keras for building the neural network model.\n",
    "#\n",
    "#```python\n",
    "## Load data\n",
    "#(train_x,), (_, _) = fashion_mnist.load_data()\n",
    "#```\n",
    "#\n",
    "#- This line loads the Fashion MNIST dataset. It contains 60,000 training images and 10,000 testing images of 10 different categories of fashion items.\n",
    "#\n",
    "#```python\n",
    "#train_x = (train_x / 255.) * 2 - 1\n",
    "#```\n",
    "#\n",
    "#- The pixel values of the images are normalized to the range [-1, 1] to facilitate training stability.\n",
    "#\n",
    "#```python\n",
    "#train_x = train_x.reshape(-1, 28, 28, 1)\n",
    "#```\n",
    "#\n",
    "#- The shape of the training data is reshaped to include a single channel for grayscale images. The original shape is (60000, 28, 28), and after reshaping, it becomes (60000, 28, 28, 1).\n",
    "#\n",
    "#```python\n",
    "## Create generator\n",
    "#generator = Sequential([\n",
    "#    Dense(512, input_shape=[100]),\n",
    "#    LeakyReLU(alpha=0.2),\n",
    "#    BatchNormalization(momentum=0.8),\n",
    "#    Dense(256),\n",
    "#    LeakyReLU(alpha=0.2),\n",
    "#    BatchNormalization(momentum=0.8),\n",
    "#    Dense(128),\n",
    "#    LeakyReLU(alpha=0.2),\n",
    "#    BatchNormalization(momentum=0.8),\n",
    "#    Dense(784),\n",
    "#    Reshape([28, 28, 1])\n",
    "#])\n",
    "#```\n",
    "#\n",
    "#- This block defines the generator model, which is a neural network composed of densely connected layers. It takes a 100-dimensional noise vector as input and outputs an image of size 28x28.\n",
    "#\n",
    "#```python\n",
    "## Create discriminator\n",
    "#discriminator = Sequential([\n",
    "#    Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1)),\n",
    "#    LeakyReLU(alpha=0.2),\n",
    "#    Dropout(0.3),\n",
    "#    Conv2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "#    LeakyReLU(alpha=0.2),\n",
    "#    Dropout(0.3),\n",
    "#    Flatten(),\n",
    "#    Dense(1, activation='sigmoid')\n",
    "#])\n",
    "#```\n",
    "#\n",
    "#- This block defines the discriminator model, which is a convolutional neural network (CNN). It takes an image of size 28x28x1 as input and outputs a single probability value indicating whether the input image is real or fake.\n",
    "#\n",
    "#```python\n",
    "## Create GAN\n",
    "#GAN = Sequential([generator, discriminator])\n",
    "#```\n",
    "#\n",
    "#- The GAN (Generative Adversarial Network) model is created by stacking the generator on top of the discriminator. This model will be used to train the generator while keeping the discriminator weights frozen.\n",
    "#\n",
    "#```python\n",
    "#discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "#```\n",
    "#\n",
    "#- The discriminator is compiled with the Adam optimizer and binary cross-entropy loss function. It aims to distinguish between real and fake images.\n",
    "#\n",
    "#```python\n",
    "#discriminator.trainable = False\n",
    "#GAN.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "#```\n",
    "#\n",
    "#- The discriminator is set to be non-trainable within the GAN model to ensure that only the generator is updated during training.\n",
    "#\n",
    "#```python\n",
    "#epochs = 5\n",
    "#batch_size = 100\n",
    "#noise_shape = 100\n",
    "#```\n",
    "#\n",
    "#- Training parameters are defined, including the number of epochs, batch size, and the dimensionality of the noise vector.\n",
    "#\n",
    "#```python\n",
    "#for epoch in range(epochs):\n",
    "#    print(f\"Currently on Epoch {epoch + 1}\")\n",
    "#```\n",
    "#\n",
    "#- This loop iterates over each epoch during training and prints the current epoch number.\n",
    "#\n",
    "#The rest of the code implements the training loop for the GAN model, where the generator and discriminator are trained alternatively to minimize their respective loss functions. Additionally, it includes visualization of generated images at specific intervals during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ababe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
